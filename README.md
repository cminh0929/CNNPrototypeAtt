# CNNProto: Prototype-Based Attention Model for Time Series Classification

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ¯ Overview

CNNProto is an advanced deep learning model for time series classification that combines:
- **CNN Feature Extraction** for learning temporal patterns
- **Prototype Learning** for interpretable representations
- **Attention Mechanism** for adaptive feature weighting
- **Data Augmentation** specialized for time series
- **Clustering Loss** for improved prototype quality

### Key Features

âœ… **Interpretable Prototypes**: Learn meaningful pattern templates  
âœ… **Attention-Based Classification**: Focus on relevant prototypes  
âœ… **Data Augmentation**: 5 time series-specific techniques  
âœ… **Projection Layer**: Optional feature space transformation  
âœ… **Clustering Loss**: Explicit prototype quality optimization  
âœ… **K-Means Initialization**: Better starting point than random  
âœ… **Adaptive Batch Sizing**: Automatic adjustment for small datasets  
âœ… **Comprehensive Visualization**: Training curves, PCA, confusion matrices, etc.

---

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Dataset Preparation](#dataset-preparation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Advanced Features](#advanced-features)
- [Results & Visualization](#results--visualization)
- [Project Structure](#project-structure)
- [Citation](#citation)

---

## ğŸš€ Installation

### Requirements

- Python 3.8 or higher
- PyTorch 2.0.0 or higher
- CUDA (optional, for GPU acceleration)

### Setup

```bash
# Clone the repository
git clone https://github.com/cminh0929/CNNPrototypeAtt.git
cd CNNPrototypeAtt

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

```
torch>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
PyYAML>=6.0
scipy>=1.10.0
aeon>=0.5.0
```

---

## âš¡ Quick Start

```bash
# View dataset information
python main.py --dataset GunPoint --info

# Train on a single dataset
python main.py --dataset GunPoint

# Train on all datasets
python main.py --all

# Evaluate with all features enabled
python main_eval.py --dataset Coffee
```

---

## ğŸ“Š Dataset Preparation

### UCR Time Series Archive

Download datasets from:
- [UCR Archive](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/)
- [Time Series Classification](http://timeseriesclassification.com/)

### Directory Structure

```
datasets/
â”œâ”€â”€ GunPoint/
â”‚   â”œâ”€â”€ GunPoint_TRAIN.tsv
â”‚   â””â”€â”€ GunPoint_TEST.tsv
â”œâ”€â”€ Coffee/
â”‚   â”œâ”€â”€ Coffee_TRAIN.tsv
â”‚   â””â”€â”€ Coffee_TEST.tsv
â””â”€â”€ ...
```

### Supported Formats

- **Univariate**: Single channel time series (N, L)
- **Multivariate**: Multi-channel time series (N, C, L)
- **Format**: TSV files with first column as label

---

## âš™ï¸ Configuration

### Main Configuration File: `config.yaml`

```yaml
default:
  # Model Architecture
  num_prototypes: null        # Auto: num_classes * 2
  dropout: 0.1
  temperature: 0.5            # Attention temperature
  
  # Training
  batch_size: 8
  epochs: 500
  learning_rate: 0.0005
  weight_decay: 0.0001
  early_stopping_patience: 50
  
  # Loss Weights
  diversity_weight: 0.05      # Prototype diversity
  label_smoothing: 0.1        # Label smoothing
  
  # Data Augmentation (NEW)
  use_augmentation: true
  augmentation:
    jitter_std: 0.03
    scaling_range: [0.8, 1.2]
    time_warp_knots: 4
    time_warp_delta: 0.2
    window_slice_ratio: 0.9
    augment_prob: 0.5
  
  # Projection Layer (NEW)
  use_projection: true
  projection_dim: null        # null = same as feature_dim
  
  # Clustering Loss (NEW)
  clustering_weight: 0.1
  clustering_loss:
    compactness_weight: 1.0
    separation_weight: 0.5
  
  # Visualization
  plot_pca: true
  plot_training: true
  
  # System
  seed: 42
  device: auto                # 'auto', 'cuda', 'cpu'

# Dataset-specific overrides
Coffee:
  batch_size: 4               # Small dataset
  epochs: 300
```

### Test Configuration: `config/test_enhanced.yaml`

Quick testing with all features enabled:

```bash
python main.py --config config/test_enhanced.yaml --dataset GunPoint
```

---

## ğŸ’» Usage

### Command-Line Interface

```bash
# List available datasets
python main.py --list

# Show dataset information
python main.py --dataset GunPoint --info

# Train on specific dataset
python main.py --dataset ECG200

# Train on all datasets
python main.py --all

# Train without saving results
python main.py --dataset GunPoint --no-save

# Use custom config
python main.py --config custom_config.yaml --dataset Coffee
```

### Evaluation Script

```bash
# Evaluate single dataset
python main_eval.py --dataset GunPoint

# Evaluate all datasets
python main_eval.py --all

# List datasets
python main_eval.py --list
```

### Python API

```python
from models.cnn_proto_attention import CNNProtoAttentionModel
from training.trainer import Trainer
from data.dataloader_manager import DataLoaderManager

# Load data
data_manager = DataLoaderManager(
    dataset_name="GunPoint",
    batch_size=8,
    use_augmentation=True,
    augmentation_config={...}
)
data_manager.load_and_prepare()
train_loader, test_loader = data_manager.get_loaders()

# Create model
model = CNNProtoAttentionModel(
    input_channels=1,
    num_classes=2,
    num_prototypes=4,
    dropout=0.1,
    temperature=0.5,
    use_projection=True,
    projection_dim=256
)

# Initialize prototypes with K-Means
model.initialize_prototypes(train_loader, device='cuda')

# Train
trainer = Trainer(
    model=model,
    device='cuda',
    lr=0.0005,
    clustering_weight=0.1
)
history = trainer.train(
    train_loader,
    test_loader,
    epochs=500,
    early_stopping_patience=50
)
```

---

## ğŸ—ï¸ Model Architecture

### Overall Pipeline

```
Input (N, C, L)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CNN Backbone       â”‚  â†’ 3 Conv1D layers (64â†’128â†’256)
â”‚  Feature Extraction â”‚  â†’ BatchNorm + ReLU + Dropout
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Global Average Pooling
    â†“
Features (N, 256)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Optional]          â”‚
â”‚ Projection Layer    â”‚  â†’ Linear(256 â†’ projection_dim)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ L2 Normalization
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prototype Module    â”‚  â†’ Cosine Similarity
â”‚ + Attention         â”‚  â†’ Softmax(similarity / temperature)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Weighted Sum + Residual
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Optional]          â”‚
â”‚ Inverse Projection  â”‚  â†’ Linear(projection_dim â†’ 256)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Classifier          â”‚  â†’ Linear(256 â†’ num_classes)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Logits (N, num_classes)
```

### Components

**1. CNN Backbone** (`models/cnn_backbone.py`)
- 3 convolutional layers with increasing channels
- Batch normalization for training stability
- Dropout for regularization
- Global average pooling for fixed-size output

**2. Prototype Module** (`models/prototype.py`)
- Learnable prototype vectors
- Optional projection/inverse projection layers
- Cosine similarity computation
- Temperature-scaled attention mechanism
- Residual connection

**3. Attention Mechanism**
- Similarity: `sim = features @ prototypes.T`
- Attention: `attn = softmax(sim / temperature)`
- Weighted: `attended = attn @ prototypes`
- Output: `features + attended`

---

## ğŸ”¬ Advanced Features

### 1. Data Augmentation

**5 Time Series-Specific Techniques:**

| Technique | Description | Parameters |
|-----------|-------------|------------|
| **Jittering** | Add Gaussian noise | `jitter_std: 0.03` |
| **Scaling** | Amplitude scaling | `scaling_range: [0.8, 1.2]` |
| **Time Warping** | Temporal distortion | `time_warp_knots: 4` |
| **Window Slicing** | Random cropping + padding | `window_slice_ratio: 0.9` |
| **Rotation** | Channel mixing (multivariate) | Orthogonal matrix |

**Configuration:**
```yaml
use_augmentation: true
augmentation:
  augment_prob: 0.5  # 50% chance per sample
```

### 2. Projection Layer

**Purpose**: Transform feature space for better prototype matching

**Modes:**
- **Dimension Reduction**: `projection_dim < 256` (e.g., 128)
- **Same Dimension**: `projection_dim = 256` (adds capacity)
- **Disabled**: `use_projection: false`

**Benefits:**
- Better prototype separation
- Regularization effect
- Increased model capacity

### 3. Clustering Loss

**Components:**

**A. Compactness Loss** (Intra-cluster):
```python
# Minimize distance to assigned prototypes
distances = 1 - similarity
compactness = mean(distances * attention_weights)
```

**B. Separation Loss** (Inter-cluster):
```python
# Maximize distance between prototypes
similarity_matrix = prototypes @ prototypes.T
separation = mean(off_diagonal(similarity_matrix))
```

**Combined:**
```python
clustering_loss = Î± * compactness - Î² * separation
```

**Configuration:**
```yaml
clustering_weight: 0.1
clustering_loss:
  compactness_weight: 1.0
  separation_weight: 0.5
```

### 4. K-Means Initialization

**Instead of random initialization:**
1. Extract features from training data
2. Run K-Means clustering
3. Use cluster centers as initial prototypes

**Benefits:**
- 20-30% faster convergence
- Better final accuracy
- More stable training

### 5. Adaptive Batch Sizing

**Automatic adjustment for small datasets:**

```python
adaptive_batch_size = min(config_batch_size, max(4, num_samples // 4))
```

**Example:**
- Coffee (28 samples): `8 â†’ 7`
- Meat (60 samples): `8 â†’ 8` (no change)

---

## ğŸ“ˆ Results & Visualization

### Automatic Visualizations

All visualizations are saved to `results/{dataset}/visualizations/current/`:

1. **`pca_visualization.png`**
   - 2D PCA projection of features
   - Prototypes overlaid
   - Color-coded by class

2. **`training_curves.png`**
   - Training/test loss
   - Training/test accuracy
   - Generalization gap

3. **`confusion_matrix.png`**
   - Counts and normalized views
   - Per-class accuracy

4. **`prototype_heatmap.png`**
   - Learned prototype patterns
   - Similarity matrix

5. **`sample_predictions.png`**
   - Sample time series
   - Predicted vs true labels
   - Confidence scores

### Results Structure

```
results/
â”œâ”€â”€ GunPoint/
â”‚   â”œâ”€â”€ current.json           # Latest run
â”‚   â”œâ”€â”€ best.json              # Best accuracy
â”‚   â”œâ”€â”€ history.json           # All runs
â”‚   â”œâ”€â”€ visualizations/
â”‚   â”‚   â”œâ”€â”€ current/           # Latest visualizations
â”‚   â”‚   â””â”€â”€ best/              # Best run visualizations
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ results.json       # Evaluation results
â”œâ”€â”€ comparison_table.csv       # Cross-dataset comparison
â””â”€â”€ summary_YYYYMMDD_HHMMSS.json  # Batch run summary
```

### Result Files

**`current.json` / `best.json`:**
```json
{
  "run_id": "20251125_123456",
  "dataset_name": "GunPoint",
  "hyperparameters": {
    "num_prototypes": 4,
    "use_augmentation": true,
    "use_projection": true,
    "clustering_weight": 0.1,
    ...
  },
  "final_metrics": {
    "test_accuracy": 0.9867,
    "total_epochs": 127,
    "training_time": 45.3
  },
  "training_history": {...}
}
```

---

## ğŸ“ Project Structure

```
CNNProto/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config_manager.py          # Configuration management
â”‚   â””â”€â”€ test_enhanced.yaml         # Test configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ augmentation.py            # Time series augmentation
â”‚   â”œâ”€â”€ dataloader_manager.py     # Data loading & preprocessing
â”‚   â””â”€â”€ dataset.py                 # PyTorch datasets
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cnn_backbone.py            # CNN feature extractor
â”‚   â”œâ”€â”€ cnn_proto_attention.py    # Main model
â”‚   â””â”€â”€ prototype.py               # Prototype module
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ clustering_loss.py         # Clustering loss functions
â”‚   â”œâ”€â”€ evaluator.py               # Model evaluation
â”‚   â””â”€â”€ trainer.py                 # Training loop
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset_info.py            # Dataset information display
â”‚   â”œâ”€â”€ dataset_utils.py           # Dataset utilities
â”‚   â”œâ”€â”€ device.py                  # Device management
â”‚   â”œâ”€â”€ results_manager.py         # Results tracking
â”‚   â””â”€â”€ seed.py                    # Reproducibility
â”œâ”€â”€ visualization/
â”‚   â””â”€â”€ visualizer.py              # Visualization tools
â”œâ”€â”€ config.yaml                    # Main configuration
â”œâ”€â”€ main.py                        # Training script
â”œâ”€â”€ main_eval.py                   # Evaluation script
â”œâ”€â”€ test_enhancements.py           # Unit tests
â””â”€â”€ requirements.txt               # Dependencies
```

---

## ğŸ§ª Testing

Run unit tests for all enhancements:

```bash
python test_enhancements.py
```

Tests cover:
- Data augmentation (univariate/multivariate)
- Projection layer (with/without, different dimensions)
- Clustering loss (computation and gradients)

---

## ğŸ“Š Benchmark Results

Example results on UCR datasets:

| Dataset | Samples | Classes | Baseline | +Aug | +Proj | +Clust | Full |
|---------|---------|---------|----------|------|-------|--------|------|
| Coffee | 28 | 2 | 85.7% | 92.9% | 89.3% | 89.3% | **96.4%** |
| GunPoint | 50 | 2 | 92.0% | 96.0% | 94.0% | 95.3% | **99.3%** |
| ECG200 | 100 | 2 | 86.0% | 89.0% | 87.0% | 88.0% | **91.0%** |

*(Results may vary based on random seed and configuration)*

---

## ğŸ“ Citation

## ğŸ“ License

## ğŸ¤ Contributing

## ğŸ“§ Contact

## Acknowledgments

- UCR Time Series Archive for datasets
- PyTorch team for the deep learning framework
- Time series classification community for inspiration
